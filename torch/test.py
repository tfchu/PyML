import torch
import torch.nn as nn
import torch.nn.functional as F

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

import numpy as np

torch.manual_seed(446)
np.random.seed(446)

# we create tensors in a similar way to numpy nd arrays
x_numpy = np.array([0.1, 0.2, 0.3])
x_torch = torch.tensor([0.1, 0.2, 0.3])
print('x_numpy, x_torch')
print(x_numpy, x_torch)
print()

# to and from numpy, pytorch
print('to and from numpy and pytorch')
print(torch.from_numpy(x_numpy), x_torch.numpy())
print()

# we can do basic operations like +-*/
y_numpy = np.array([3,4,5.])
y_torch = torch.tensor([3,4,5.])
print("x+y")
print(x_numpy + y_numpy, x_torch + y_torch)
print()

# many functions that are in numpy are also in pytorch
print("norm")
print(np.linalg.norm(x_numpy), torch.norm(x_torch))
print()

# to apply an operation along a dimension,
# we use the dim keyword argument instead of axis
'''
0-th dim: across rows (1, 3), (2, 4)
1-th dim: across cols (1, 2), (3, 4)
[
    [1, 2]
    [3, 4]
]
'''
print("mean along the 0th dimension")
x_numpy = np.array([[1,2],[3,4.]])
x_torch = torch.tensor([[1,2],[3,4.]])
print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0))
print(np.mean(x_numpy, axis=1), torch.mean(x_torch, dim=1))

# "MNIST"
N, C, W, H = 10000, 3, 28, 28   # number of samples, channel, width, height
X = torch.randn((N, C, W, H))

print(X.shape)
print(X.view(N, C, 784).shape)
print(X.view(-1, C, 784).shape) # automatically choose the 0th dimension with -1, only one dim can be -1

# PyTorch operations support NumPy Broadcasting Semantics.
# broadcast when dim does not exist, or dim = 1
# final dim: larger dim of all tensors
# original: reshape y to 1,3,1,1, and copy 5 times, then x + y
'''
x is 5 of these
tensor([[[[0.],
          [0.],
          [0.],
          [0.]]],
        [[[0.],
          [0.],
          [0.],
          [0.]]],
        ...
'''
x=torch.empty(5,1,4,1)
y=torch.empty(  3,1,1)
print((x+y).size()) # [5, 3, 4, 1]


a = torch.tensor(2.0, requires_grad=True) # we set requires_grad=True to let PyTorch know to keep the graph
b = torch.tensor(1.0, requires_grad=True)
c = a + b
d = b + 1
e = c * d
print('c', c)
print('d', d)
print('e', e)